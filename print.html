<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>HyperMake</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="A parameterized pipeline manager">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded "><a href="install.html"><strong aria-hidden="true">1.</strong> Installation</a></li><li class="chapter-item expanded "><a href="tutorial/tutorial-index.html"><strong aria-hidden="true">2.</strong> Tutorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tutorial/hello.html"><strong aria-hidden="true">2.1.</strong> Hello, world!</a></li><li class="chapter-item expanded "><a href="tutorial/params.html"><strong aria-hidden="true">2.2.</strong> Parameters</a></li><li class="chapter-item expanded "><a href="tutorial/compose.html"><strong aria-hidden="true">2.3.</strong> Task composition</a></li><li class="chapter-item expanded "><a href="tutorial/pack.html"><strong aria-hidden="true">2.4.</strong> Packages</a></li><li class="chapter-item expanded "><a href="tutorial/decorators.html"><strong aria-hidden="true">2.5.</strong> Decorators</a></li><li class="chapter-item expanded "><a href="tutorial/reduce.html"><strong aria-hidden="true">2.6.</strong> Reduction</a></li><li class="chapter-item expanded "><a href="tutorial/plans.html"><strong aria-hidden="true">2.7.</strong> Plans</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Language reference</li><li class="chapter-item expanded "><a href="lang/modules.html"><strong aria-hidden="true">3.</strong> Modules</a></li><li class="chapter-item expanded "><a href="lang/decorators.html"><strong aria-hidden="true">4.</strong> Decorators</a></li><li class="chapter-item expanded "><a href="lang/packages.html"><strong aria-hidden="true">5.</strong> Packages</a></li><li class="chapter-item expanded "><a href="lang/filesys.html"><strong aria-hidden="true">6.</strong> File systems</a></li><li class="chapter-item expanded affix "><li class="part-title">Library reference</li><li class="chapter-item expanded "><a href="stdlib/stdlib.html"><strong aria-hidden="true">7.</strong> Standard library</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stdlib/std.html"><strong aria-hidden="true">7.1.</strong> std</a></li><li class="chapter-item expanded "><a href="stdlib/aws.html"><strong aria-hidden="true">7.2.</strong> aws</a></li><li class="chapter-item expanded "><a href="stdlib/az.html"><strong aria-hidden="true">7.3.</strong> az</a></li><li class="chapter-item expanded "><a href="stdlib/conda.html"><strong aria-hidden="true">7.4.</strong> conda</a></li><li class="chapter-item expanded "><a href="stdlib/ssh.html"><strong aria-hidden="true">7.5.</strong> ssh</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">HyperMake</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="hypermake"><a class="header" href="#hypermake">HyperMake</a></h1>
<p><code>HyperMake</code> is a parameterized pipeline definition language (think of a <code>make</code> where tasks can be parameterized) heavily inspired by <a href="https://github.com/jhclark/ducttape">Ducttape</a>.</p>
<ul>
<li><strong>Shell scripting</strong>: Write tasks in plain Bash, just like <code>make</code>. No Python or YAML for defining tasks.</li>
<li><strong>Cached intermediate results</strong>: Intermediate results are cached, so that if a task fails, it can be re-run from the last successful task.</li>
<li><strong>Parameterization of tasks:</strong> a task can be of multiple versions (e.g. in a ML pipeline, a task can be of different hyperparameters).</li>
<li><strong>Minimal juggling</strong>: Inputs and outputs are just files/symlinks, and arguments are all passed as environment variables. Tasks are realized as a child process.</li>
<li><strong>Automatic parallelization:</strong> based on the dependency DAG.</li>
<li><strong>Cloud-agnostic:</strong> tasks can be run locally, or on a cloud (e.g. AWS, Azure), with minimal changes to the pipeline.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<h3 id="via-homebrew"><a class="header" href="#via-homebrew">Via Homebrew</a></h3>
<p>HyperMake can be installed via <a href="https://brew.sh"><code>brew</code></a> with a custom tap.</p>
<pre><code class="language-shell">brew tap ctongfei/repo
brew install --HEAD ctongfei/repo/hypermake
</code></pre>
<p>Right now the Homebrew formula is configured to use the <code>HEAD</code> version of HyperMake, which is the latest version on the <code>main</code> branch. To reinstall if the latest version changed, do</p>
<pre><code class="language-shell">brew reinstall ctongfei/repo/hypermake
</code></pre>
<h3 id="build-from-source"><a class="header" href="#build-from-source">Build from source</a></h3>
<p>HyperMake can also be directly built from source. It requires <a href="https://www.scala-sbt.org"><code>sbt</code></a> to build.</p>
<pre><code class="language-shell">git clone https://github.com/ctongfei/hypermake
cd hypermake

make
make install
</code></pre>
<p>This will build the HyperMake binary and install it locally to <code>$HOME/.local/bin/hypermake</code>.
To install it elsewhere, simply modify the <code>$PREFIX</code> variable in the <code>Makefile</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><p>In the following sections we will first glimpse into HyperMake by running a simple "Hello, world" task.</p>
<p>Then, we will gradually introduce more advanced features of HyperMake to build a pipeline for
running the <a href="https://beir.ai">BEIR</a> (<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html">paper</a>) benchmark<sup class="footnote-reference"><a href="#beir">1</a></sup>.</p>
<div class="footnote-definition" id="beir"><sup class="footnote-definition-label">1</sup>
<p>BEIR is a robust and heterogeneous evaluation benchmark for zero-shot information retrieval. It includes a diverse set of retrieval tasks, such as web search, question answering, and entity retrieval. The benchmark is designed to evaluate the generalization capabilities of retrieval models across different tasks and domains.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hello-world"><a class="header" href="#hello-world">Hello, world!</a></h1>
<p>To introduce HyperMake, let's define our first task:</p>
<pre><code class="language-makefile">task hello:
  echo "Hello, world!"
</code></pre>
<p>Save this file as <code>hello.hm</code>.</p>
<p>We have created our first HyperMake script file that contains a single task.
This script defines a task named <code>hello</code> that prints <code>Hello, world!</code> to the console. There is no input or output for this task.</p>
<blockquote>
<p>Note the syntax here: A code block starts after the <code>:</code> at the end of the task signature.
A code block is a consecutive list of <em><strong>indented</strong></em> lines of scripts, where each line must start with at least <em><strong>2 spaces</strong></em>.
By default, the script is written in Bash.</p>
</blockquote>
<p>If you are familiar with <code>make</code>, you can think of a task as a <code>make</code> rule. The task above written as a Makefile would just be</p>
<pre><code class="language-makefile">hello:
    echo "Hello, world!"

.PHONY: hello  # since this task does not produce any output
</code></pre>
<p>Now let's run this task!</p>
<p>Execute the following command in your shell:</p>
<pre><code class="language-shell">  hypermake hello.hm run hello 
</code></pre>
<p>We should see the output "Hello, world!" printed in the terminal.</p>
<blockquote>
<p>The basic command line usage is <code>hypermake $script &lt;subtask&gt; $target</code>. Here the <code>&lt;subtask&gt;</code> is simply <code>run</code>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parameters"><a class="header" href="#parameters">Parameters</a></h1>
<p>For the following tutorial sections, we will gradually build our <code>beir.hm</code> pipeline.</p>
<p>In any ML pipeline, one starts with data. In this example, we will use the BEIR-14 subset (those with public licenses) of the BEIR benchmark.</p>
<p>We declare these datasets as a pipeline <strong>parameter</strong> <code>BeirDataset</code>:</p>
<pre><code class="language-python">beir_dataset = {BeirDataset: 
    msmarco scifact trec-covid webis-touche2020 
    fiqa dbpedia-entity fever nfcorpus hotpotqa 
    climate-fever scidocs nq quora arguana
}
</code></pre>
<blockquote>
<p>Note the syntax for declaring a parameter: <code>{ParamName: key0 key1 ...}</code>. For each parameter, <strong>the first key</strong> is considered the <strong>default case</strong> -- here <code>msmarco</code>.</p>
</blockquote>
<p>Now we want to download the raw data from their official location.</p>
<pre><code class="language-python">beir_url_prefix = "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets"
</code></pre>
<p>We proceed to write the first task of our pipeline: downloading the raw data and unzip it.</p>
<pre><code class="language-bash">task raw_beir_data(beir_dataset=$, beir_url_prefix=$) -&gt; out:
  wget -O dataset.zip $beir_url_prefix/$beir_dataset.zip
  unzip dataset.zip
  rm dataset.zip
  mv $beir_dataset out
</code></pre>
<p>We declared a task that takes two inputs: <code>beir_dataset</code> and <code>beir_url_prefix</code>, and produces an output directory <code>out</code>.</p>
<blockquote>
<p>The syntax <code>name=$</code> is a shorthand for <code>name=$name</code>. Here, <code>beir_dataset=$</code> introduced the <code>beir_dataset</code> parameter as an input to the task.</p>
</blockquote>
<p>The task is considered <em>complete</em> when its output directory <code>out</code> exists after the task exits with a zero status code.</p>
<blockquote>
<p>In HyperMake, the success of a task is determined by</p>
<ul>
<li>the existence of all of its specified outputs</li>
<li><strong>AND</strong> the zero exit status code of the task script.</li>
</ul>
</blockquote>
<p>Note that</p>
<ul>
<li><code>beir_dataset</code> is a <strong>parameterized value</strong>: it can take any of the values in the <code>BeirDataset</code> parameter;</li>
<li><code>beir_url_prefix</code> is a <em>singleton</em> (non-parameterized) value.</li>
</ul>
<p>Hence the task <code>raw_beir_data</code> is parameterized with <strong>all the parameters in its inputs</strong>. Consider
<code>raw_beir_data</code> not a single task, but a <strong>tensor</strong> of tasks, where it has dimension 1: the <code>BeirDataset</code> parameter.</p>
<h3 id="invoking-the-task"><a class="header" href="#invoking-the-task">Invoking the task</a></h3>
<p>Let's invoke this task. To download the <code>msmarco</code> dataset, we run:</p>
<pre><code class="language-bash">hypermake beir.hm run "raw_beir_data[BeirDataset:msmarco]"
</code></pre>
<p>We will find the downloaded dataset in the <code>out/raw_beir_data/default</code> directory.</p>
<blockquote>
<p>Note the task indexing syntax: <code>task[Param0: key0, Param0: key1, ...]</code>.</p>
</blockquote>
<p>Download another one:</p>
<pre><code class="language-bash">hypermake beir.hm run "raw_beir_data[BeirDataset:scifact]"
</code></pre>
<p>We will find the downloaded dataset in the <code>out/raw_beir_data/BeirDataset=scifact</code> directory.</p>
<blockquote>
<p>The output directory will be <code>out/&lt;task-name&gt;/&lt;non-default-params&gt;</code>, where <code>&lt;non-default-params&gt;</code> is the URL percent-encoding of the key-value pairs.</p>
</blockquote>
<p>Clearly we do not wish to invoke the downloading one by one. Let's use the wildcard <code>*</code>:</p>
<pre><code class="language-bash">hypermake beir.hm run "raw_beir_data[BeirDataset: *]" -j8
</code></pre>
<blockquote>
<p>The <code>-j</code> flag specifies the number of parallel jobs to run. Here we run 8 jobs in parallel.</p>
</blockquote>
<p>At this point we have downloaded all the datasets. We will proceed to the next steps in the following sections.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="task-composition"><a class="header" href="#task-composition">Task composition</a></h1>
<p>In the previous section, we saw how to define a task with parameters. In this section, we will see how to compose tasks together to form a pipeline.</p>
<p>To run the BEIR benchmark with the Pyserini package, we envision the following pipeline:</p>
<pre class="mermaid">graph LR;
raw_beir_data --&gt; beir_to_trec;
beir_to_trec --&gt; index;
index --&gt; retrieve;
beir_to_trec --&gt; retrieve;
retrieve --&gt; evaluate;
beir_to_trec --&gt; evaluate;
</pre>
<p>Essentially, our pipeline consists of the following steps:</p>
<ul>
<li>Download the raw data in <code>raw_beir_data</code>;</li>
<li>Preprocess the data to the standard TREC format in <code>beir_to_trec</code>;</li>
<li>Index the data in <code>index</code> to create a BM25 index (depending on the preprocesed data);</li>
<li>Retrieve the top-100 documents for each query in <code>retrieve</code> (depending on the index and the data);</li>
<li>Evaluate the retrieval results in <code>evaluate</code> (depending on the data and the retrieved results).</li>
</ul>
<p>To compose these tasks, we need to define the dependencies between them. This is done by specifying the output of one task as the input of another task.</p>
<pre><code class="language-python">task raw_beir_data(beir_dataset=$, beir_url_prefix=$) -&gt; out:
  ...
  
task beir_to_trec(data=$raw_beir_data.out) -&gt; out:
  ...

task index(data=$beir_to_trec.out) -&gt; out:
  ...

task retrieve(data=$beir_to_trec.out, index=$index.out) -&gt; out:
  ...

task evaluate(data=$raw_beir_data.out, rseult=$retrieve.out) -&gt; out:
  ...
</code></pre>
<p>In the next sections we will implement these tasks one by one.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="packages"><a class="header" href="#packages">Packages</a></h1>
<p>Our previous sketch requires some packages to be built.</p>
<ul>
<li>A <code>conda</code> package that contains a bunch of Python libraries (mainly <a href="https://github.com/castorini/pyserini">Pyserini</a>) to run BM25 search;</li>
<li>The NIST <a href="https://github.com/usnistgov/trec_eval"><code>trec_eval</code></a> package to evaluate the retrieval results.</li>
</ul>
<p>We will define these packages in HyperMake and let them be part of the whole pipeline,
so when a user runs the pipeline, the packages will be built and installed automatically.</p>
<p>A package in HyperMake is defined with the <code>package</code> keyword, and it is a special kind of task.</p>
<h3 id="creating-a-conda-package"><a class="header" href="#creating-a-conda-package">Creating a Conda package</a></h3>
<pre><code class="language-python">package pyserini -&gt; out:
  mkdir -p $out
  conda create -y \
    -p $out \
    -c conda-forge \
    python=3.10 openjdk=21
  $out/bin/pip install torch faiss-cpu pyserini
</code></pre>
<p>We declared a package named <code>pyserini</code> that when building, creates a new Conda environment with Python 3.10 and OpenJDK 21, and installs Pyserini in it. Note that we build the package in a HyperMake-managed, separate directory <code>$out</code> (with the <code>-p</code>/<code>--prefix</code> directive of Conda) instead of a global Conda environment.</p>
<p>This is common so that HyperMake provided standard library subroutines to make this easier:</p>
<pre><code class="language-python">import conda
package pyserini = conda.create(
    packages="python=3.10 openjdk=21",
    extra_args="-c conda-forge",
    extra_pip_packages="torch faiss-cpu pyserini"
)
</code></pre>
<h3 id="building-the-trec_eval-package"><a class="header" href="#building-the-trec_eval-package">Building the <code>trec_eval</code> package</a></h3>
<p><code>trec_eval</code> is a C package built with Make. We can define a package for it as well:</p>
<pre><code class="language-python">package trec_eval -&gt; out:
  git clone https://github.com/usnistgov/trec_eval.git $out
  cd $out
  make
</code></pre>
<blockquote>
<p>In HyperMake, <em><strong>a package must have exactly 1 output</strong></em>: the built package directory. To refer to the output directory, directly use the package name as a variable (e.g. <code>$pyserini</code>, <code>$trec_eval</code> here).</p>
</blockquote>
<blockquote>
<p>What is exactly the difference between a package and a task?
When a HyperMake pipeline is defined across multiple file systems (e.g. local, AWS, SSH, etc.),
a task is only run once and transferred between file systems, while a package is separately built on each file system.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decorators"><a class="header" href="#decorators">Decorators</a></h1>
<p>Now let's convert our downloaded BEIR data to the standard TREC format.
This format is standard for information retrieval tasks.
There are 3 kinds of files in the TREC format:</p>
<ul>
<li><code>*.queries</code>, a TSV file with two columns, query id and query text;</li>
<li><code>*.qrels</code>, a TSV file with four columns, query id, iteration id, document id, and relevance label;</li>
<li><code>corpus</code>, a TSV file with two columns, document id and document text.</li>
</ul>
<p>This conversion involves some complex processing, so we will first write a Python script <code>beir_to_trec.py</code> to do this.</p>
<pre><code class="language-python">import os
import json
import sys
import csv
from tqdm import tqdm
data = sys.argv[1]  # The directory containing the downloaded BEIR data
out = sys.argv[2]  # The directory to write the TREC format data
os.mkdir(out)
with open(f"{data}/corpus.jsonl") as f_in, open(f"{out}/corpus", 'w') as f_out:
  for line in tqdm(f_in):
    obj = json.loads(line)
    id = obj['_id']
    text = obj['text'].replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
    title = obj.get('title', "")
    trec_line = f"{id}\t{title}: {text}" if title != "" else f"{id}\t{text}" 
    # Concatenate title and text
    print(trec_line, file=f_out)
queries = {}
with open(f"{data}/queries.jsonl") as f:
  for line in tqdm(f):
    obj = json.loads(line)
    id = obj['_id']
    text = obj['text']
    queries[id] = text
for partition in os.listdir(f"{data}/qrels"):
  partition = os.path.splitext(partition)[0]
  with open(f"{data}/qrels/{partition}.tsv") as f_in, open(f"{out}/{partition}.qrels", 'w') as f_out:
    query_ids = set()
    for row in tqdm(csv.DictReader(f_in, delimiter='\t')):
      query_ids.add(row['query-id'])
      print(f"{row['query-id']}\t0\t{row['corpus-id']}\t{row['score']}", file=f_out)
  with open(f"{out}/{partition}.queries", 'w') as f:
    for query_id in query_ids:
      print(f"{query_id}\t{queries[query_id]}", file=f)
</code></pre>
<p>Now we can write a task to run this script.</p>
<pre><code class="language-bash">task beir_to_trec(data=$raw_beir_data.out) -&gt; out:
  python beir_to_trec.py $data out
</code></pre>
<p>This task takes the output of the <code>raw_beir_data</code> task as input and produces a directory <code>out</code> containing the TREC format data.</p>
<p>But to run this task, before invoking <code>hypermake</code> from the command line, we need to first activate the Conda environment that contains the Python dependencies required by the script <code>beir_to_trec.py</code>. This is not ideal -- recall that we just built the <code>pyserini</code> Conda environment in the previous section. We would like to run this task in the <code>pyserini</code> environment.</p>
<p>Let's decorate this task with a <code>@conda</code> <strong>decorator</strong> that activates the <code>pyserini</code> environment.</p>
<pre><code class="language-python">import conda

@conda.activate(environment=$pyserini)
task beir_to_trec(data=$raw_beir_data.out) -&gt; out:
    python beir_to_trec.py $data out
</code></pre>
<blockquote>
<p>What is the magic behind this decorator? A HyperMake decorator takes a script and returns a new wrapped script.
To implement your own decorator, you need an object with a <code>run</code> function.
If you are curious, you can find the implementation of the <code>conda.activate</code> decorator <a href="https://github.com/ctongfei/hypermake/blob/main/src/main/hypermake/conda.hm">here</a>.</p>
</blockquote>
<h3 id="next-steps"><a class="header" href="#next-steps">Next steps</a></h3>
<p>Let's continue building our pipeline, starting from indexing the corpus with Pyserini.</p>
<p>At this step, we run a Bash script under the Pyserini conda environment.</p>
<pre><code class="language-python">@conda.activate(environment=$pyserini)
task index(data=$beir_to_trec.out) -&gt; out:
  mkdir corpus
  cat $data/corpus \
    | jq -Rc 'inputs | split("\t") | {id: .[0], contents: .[1]}' \
    &gt; corpus/corpus.json  # Convert TREC format to Pyserini JSON
  python -m pyserini.index.lucene \
    --collection JsonCollection \
    --input corpus \
    --index $out \
    --generator DefaultLuceneDocumentGenerator \
    --threads $(nproc) \
    --storePositions \
    --storeDocvectors \
    --storeRaw
</code></pre>
<p>Run the actual retrieving with Pyserini.</p>
<pre><code class="language-python">@conda.activate(environment=$pyserini)
task retrieve(
  data=$beir_to_trec.out, 
  test_partition=$, 
  index=$index.out
) -&gt; (out="result.qres"):
  ln -s $data/$test_partition.queries test.tsv
  python -m pyserini.search.lucene \
    --index $index \
    --topics test.tsv \
    --output $out \
    --batch-size 32 \
    --hits 100 \
    --threads $(nproc) \
    --remove-duplicates --remove-query --bm25
</code></pre>
<p>Evaluate the retrieval results with <code>trec_eval</code>.</p>
<h3 id="step-7-evaluate-the-retrieval-results-with-trec_eval"><a class="header" href="#step-7-evaluate-the-retrieval-results-with-trec_eval">Step 7: Evaluate the retrieval results with <code>trec_eval</code></a></h3>
<pre><code class="language-bash">task evaluate(
  data=$beir_to_trec.out,
  result=$retrieve.out,
  test_partition=$,
  trec_eval=$
) -&gt; (out="eval.txt"):
  $trec_eval/trec_eval -m all_trec $data/$test_partition.qrels $result &gt; $out
</code></pre>
<blockquote>
<p>Here we referenced the output of the <code>trec_eval</code> package as <code>$trec_eval</code>. This is because the <code>trec_eval</code> package is a separate package that we built in the previous section.
We can refer to the output of a package directly by its name.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reduction"><a class="header" href="#reduction">Reduction</a></h1>
<p>At this point we have a pipeline that retrieves documents from a corpus using Pyserini. We have also evaluated the retrieval results with <code>trec_eval</code>. But there are a lot of runs: one for each dataset in BEIR-14. We would like to aggregate the evaluation results.</p>
<p>This is done by <strong>reduction</strong> in HyperMake: think this as <code>reduce</code> in functional programming, or <code>.max(dim=i)</code> in a tensor processing library.</p>
<p>Recall that at the end of the pipeline we have built thus far, the <code>trec_eval</code> output files are in the format of <code>eval.txt</code> for each dataset, under the variable <code>$evaluate.out</code>. We would like to aggregate the results over all datasets in BEIR-14. Additionally, there is more than 1 metric that we cared: for example, <code>ndcg_cut_10</code>, <code>recall_100</code>, <code>map</code>, and <code>mrr</code>.</p>
<p>We can define a new task <code>aggregate_metric</code> that takes the evaluation results of all datasets and aggregates them. The task definition is as follows:</p>
<pre><code class="language-python">metric = {Metric: ndcg_cut_10 recall_100 map mrr}

task aggregate_metric(
  eval_results=$evaluate[BeirDataset: *].out, 
  metric=$
) -&gt; (out="aggregate.txt"):
  grep -E "^$metric " $eval_results/* &gt; $out
</code></pre>
<p>Note here that we used <code>$evaluate[BeirDataset: *].out</code> to refer to the outputs of all <code>evaluate</code> tasks for each dataset in BEIR-14. The parameter <code>eval_results</code>, while logically is a dictionary from configurations to files, will be realized as a folder of files for the Shell script.</p>
<blockquote>
<p>HyperMake maps a dictionary to a folder of files in the Shell script. This is a common pattern in HyperMake to handle multiple outputs.</p>
<ul>
<li><code>dict[key]</code> would be <code>$d/$key</code> in Shell.</li>
<li><code>dict.values()</code> would be <code>$d/*</code> in Shell.</li>
<li><code>for key in dict</code> would be <code>for f in $d/*</code> in Shell.</li>
</ul>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="plans"><a class="header" href="#plans">Plans</a></h1>
<p>We have now built a full pipeline for the BEIR-14 dataset. The pipeline is defined in <code>beir.hm</code>.</p>
<p>Let's first preview this in the command line:</p>
<pre><code class="language-shell">hypermake beir.hm list
</code></pre>
<p>It shows a rendition of the DAG structure of our pipeline:</p>
<pre><code>HyperMake 0.1.0 -- A parameterized pipeline manager
Workflow file: beir.hm

Variables:
  • Metric: { ndcg_cut_10 recall_100 map mrr }
  • BeirDataset: { msmarco scifact trec-covid webis-touche2020 fiqa dbpedia-entity fever nfcorpus hotpotqa climate-fever scidocs nq quora arguana }

Tasks:
  • pyserini@local
  │ • raw_beir_data[BeirDataset]
  │ │ • trec_eval@local
  ├─┴─│─• beir_to_trec[BeirDataset]
  ├───│─┼─• index[BeirDataset]
  └───│─┼─┴─• retrieve[BeirDataset]
      └─┴───┴─• evaluate[BeirDataset]
              └─• aggregate_metric[Metric]
</code></pre>
<p>To run them all:</p>
<pre><code class="language-shell">hypermake beir.hm run "aggregate_metric[Metric: *]" -j8
</code></pre>
<p>Here we compute the <code>aggregate_metric</code> task for all metrics defined in <code>metric</code>, with max 8 jobs running in parallel!</p>
<p>Or, we can define a <strong>plan</strong> to define the targets we want to run:</p>
<pre><code class="language-python">plan RunBEIR = {
    aggregate_metric[Metric: *]
}
</code></pre>
<blockquote>
<p>A plan definition can contain multiple targets, separated by commas.</p>
</blockquote>
<p>And invoke it with:</p>
<pre><code class="language-shell">hypermake beir.hm run RunBEIR -j8
</code></pre>
<p>The results should match the results in Table 2 (BM25 column) of the <a href="https://arxiv.org/pdf/2310.08319">RepLlama paper</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classesobjects"><a class="header" href="#classesobjects">Classes/Objects</a></h1>
<p>In Hypermake, sometimes it is needed to bundle certain definitions together so that it can be reused.
This forms an <code>object</code> (or <em>modules</em>, since a module can be seen as a singleton object):</p>
<pre><code>object my_obj:
  def x(...):
  def y(...):
  
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decorators-1"><a class="header" href="#decorators-1">Decorators</a></h1>
<p>In Hypermake, a task can be decorated with some decorators, effectively modifying its behavior. This can support</p>
<ul>
<li>Running with different shell;</li>
<li>Running in specific virtual environments;</li>
<li>Running through some cluster submission systems;</li>
<li>etc.</li>
</ul>
<p>A decorator in HyperMake is just an object with a <code>run</code> method that takes a script as input and runs a modified version.</p>
<pre><code class="language-py">object decorator:
    def run(internal_script):
        ...
</code></pre>
<p>If a decorator admits parameters, it simply becomes a class:</p>
<pre><code class="language-py">class decorator(args):
    def run(internal_script):
        ...
</code></pre>
<p>and when applying a decorator, one could write</p>
<pre><code class="language-shell">@decorator(args)
task taskName(...) -&gt; out:
  ...
</code></pre>
<h4 id="example-1-a-decorator-that-runs-a-task-in-python"><a class="header" href="#example-1-a-decorator-that-runs-a-task-in-python">Example 1: A decorator that runs a task in Python</a></h4>
<p>An example that let us runs a task in Python instead of shell:</p>
<pre><code class="language-shell">object python:
  def run(internal_script):
    python $internal_script

@python
task helloWorldInPython:
  print("Hello World" + " " + "in Python!")
</code></pre>
<p>There is no need to define this in your pipelines: it is already available in the standard library as <code>@std.run(interpreter="python")</code>.</p>
<h4 id="example-2-decorates-a-script-to-run-in-a-conda-virtual-environment"><a class="header" href="#example-2-decorates-a-script-to-run-in-a-conda-virtual-environment">Example 2: Decorates a script to run in a Conda virtual environment</a></h4>
<p>In Python, a task can be run in different Conda virtual environments. This is a decorator that lets us do that.</p>
<pre><code class="language-shell">class conda(env):
  def run(internal_conda_script):
    eval "$(command conda 'shell.bash' 'hook' 2&gt; /dev/null)"
    conda activate $env
    . $internal_conda_script
    conda deactivate

@conda(env={Env: base myenv})
task helloWorldFromEnv:
  python -c "print('Hello World in Python from $env!')"
</code></pre>
<p>Note that in the task <code>helloWorldFromEnv</code>, the decorator <code>conda</code> has a parameterized argument: <code>env={Env: base myenv}</code>.
We can invoke both cases of the task <code>helloWorldFromEnv</code>:</p>
<pre><code class="language-shell">hypermake tutorial/decorators.hm run 'helloWorldFromEnv[Env: *]'
</code></pre>
<p>We will see both lines</p>
<pre><code>Hello World in Python from base!
Hello World in Python from myenv!
</code></pre>
<p>output to the terminal.</p>
<h4 id="example-3-chaining-decorators"><a class="header" href="#example-3-chaining-decorators">Example 3: Chaining decorators</a></h4>
<p>We have now created two decorators:</p>
<ul>
<li><code>@python</code> that executes a script using Python instead of Bash as the interpreter;</li>
<li><code>@conda</code> that runs a task in a specific Conda virtual environment.</li>
</ul>
<p>Can we compose these decorators? Yes.</p>
<pre><code class="language-python">@conda(env={Env: base myenv})
@python
task helloWorldInPythonFromEnv:
  import os
  print(f"Hello World in Python from {os.environ['env']}!")
</code></pre>
<blockquote>
<p>One can use <code>os.environ[var]</code> to get the environment variable <code>$var</code> in Python.
First, our script is wrapped by <code>@python</code>, then <code>@conda(env)</code>.
Recall that HyperMake passes parameters into the script as environment variables:
we cannot use <code>$env</code> to get the HyperMake variable in Python.</p>
</blockquote>
<h4 id="example-4-a-decorator-that-runs-a-compiled-language-c"><a class="header" href="#example-4-a-decorator-that-runs-a-compiled-language-c">Example 4: A decorator that runs a compiled language: C</a></h4>
<p>We can also create a decorator that runs a task in C. Since C is a compiled language, we need to compile the script first.</p>
<pre><code class="language-python">object gcc:
  def run(internal_c_script):
    ln -s $internal_c_script source.c
    gcc source.c -o source.out
    ./source.out
</code></pre>
<p>Now we can do fun things: write C scripts in HyperMake!</p>
<pre><code class="language-c">@gcc
task print(input="abcde"):
  #include &lt;stdio.h&gt;
  #include &lt;stdlib.h&gt;
  int main() {
    char* input = getenv("input");
    printf("%s\n", input);
    return 0;
  }
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="packages-1"><a class="header" href="#packages-1">Packages</a></h1>
<p>In HyperMake, packages are special tasks that builds a software package.
They can depends on other packages but not tasks, and will be built differently on different environments (see next tutorial).</p>
<p>A package is defined as follows (note that a package can only have exactly 1 output):</p>
<pre><code>package $packageName -&gt; $packageOutputName:
  # build script
</code></pre>
<p>For example, let's build <a href="https://github.com/usnistgov/trec_eval"><code>trec_eval</code></a> (a standard information retrieval evaluation toolkit from NIST)
from its C source code:</p>
<pre><code>package trec_eval -&gt; out:
  mkdir -p $out
  git clone https://github.com/usnistgov/trec_eval $out
  cd $out
  make
</code></pre>
<p>Here we clone the repository into a HyperMake-managed directory <code>$out</code>, and then run <code>make</code> to build the package. The binary will be built in <code>$out</code>.</p>
<p>To refer to this  package output, use <code>$trec_eval</code> (there is no need to specify <code>$trec_eval.out</code>).
For example, if an evaluation task requires this package, one can write</p>
<pre><code>task eval(trec_eval=$, pred=$, gold=$) -&gt; out:
  $trec_eval/trec_eval $gold $pred &gt; $out
</code></pre>
<h4 id="example-1-copying-a-package-from-a-local-directory"><a class="header" href="#example-1-copying-a-package-from-a-local-directory">Example 1: Copying a package from a local directory</a></h4>
<pre><code class="language-bash">package pack1 -&gt; out:
  ln -s $localDir $out
</code></pre>
<p>This behavior can be written as</p>
<pre><code class="language-bash">import std
package pack1 = std.symlink(path=$localDir)
</code></pre>
<h4 id="example-2-cloning-from-a-remote-repository-and-build-it"><a class="header" href="#example-2-cloning-from-a-remote-repository-and-build-it">Example 2: Cloning from a remote repository and build it</a></h4>
<pre><code class="language-bash">package pack2(repo=$) -&gt; out:
  git clone $repo out
  cd out
  make
</code></pre>
<h4 id="example-3-creates-a-conda-environment-from-a-python-package"><a class="header" href="#example-3-creates-a-conda-environment-from-a-python-package">Example 3: Creates a Conda environment from a Python package</a></h4>
<pre><code class="language-bash">package pack3(pythonPackage=$) -&gt; out:
  mkdir -p $out
  conda env create -p $out -f $pythonPackage/environment.yml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="file-systems"><a class="header" href="#file-systems">File systems</a></h1>
<p>A <strong>file system</strong> encapsulates the operations that can be performed on files and directories in a particular environment in HyperMake.</p>
<p>HyperMake provides a default file system implementation for the local file system (<code>local</code>),
and has utilities to define file systems over common remote systems such as SFTP, AWS S3, and Azure Blob Storage.</p>
<p>Additionally, it is possible to define custom file systems for different environments.</p>
<p>In HyperMake, a file system is an <em>object</em> with various member functions defined.</p>
<h3 id="functions-in-a-file-system-object"><a class="header" href="#functions-in-a-file-system-object">Functions in a file system object</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Member</th><th>Description</th></tr></thead><tbody>
<tr><td><code>fs.root</code></td><td>A string specifying the root path of all HyperMake outputs.</td></tr>
<tr><td><code>fs.read(file)</code></td><td>Reads the file <code>$file</code> and outputs the content to <code>stdout</code>.</td></tr>
<tr><td><code>fs.mkdir(dir)</code></td><td>Creates an empty directory <code>$dir</code>. <br> This should have the semantics of <code>mkdir -p</code>: it should create all parent <br> directories if they do not exist, and it should not fail if the directory <br> already exists.</td></tr>
<tr><td><code>fs.exists(file)</code></td><td>Checks if <code>$file</code> exists in <code>fs</code>.</td></tr>
<tr><td><code>fs.link(src, dst)</code></td><td>Creates a symbolic link at <code>$dst</code> that links to <code>$src</code>.</td></tr>
<tr><td><code>fs.touch(file)</code></td><td>Creates an empty file at path `$file</td></tr>
<tr><td><code>fs.remove(file)</code></td><td>Removes file <code>$file</code> in <code>fs</code>. <br> If <code>$file</code> is a directory, it should remove the directory and all its contents.</td></tr>
<tr><td><code>fs.upload(src, dst)</code></td><td>Uploads the file or directory <code>$src</code> in <code>local</code> to <code>$dst</code> in <code>fs</code>.</td></tr>
<tr><td><code>fs.download(src, dst)</code></td><td>Downloads the file or directory <code>$src</code> in <code>fs</code> to <code>$dst</code> in <code>local</code>.</td></tr>
<tr><td><code>fs.execute(command)</code></td><td><strong>(Optional)</strong> Executes the command <code>$command</code> in <code>fs</code>'s shell. <br> This can be omitted if the file system does not support running commands.</td></tr>
</tbody></table>
</div>
<p>There is no need to define <code>local</code> as it is internal to HyperMake. A reference implementation of <code>local</code> is provided below.</p>
<pre><code class="language-py">object local:
    root = "."
    
    def read(file):
        cat $file
    
    def mkdir(dir):
        mkdir -p $dir
    
    def exists(file):
        test -e $file
    
    def link(src, dst):
        ln -s $src $dst
    
    def touch(file):
        touch $file
    
    def remove(file):
        rm -r $file
       
    def upload(src, dst):
        ln -s $src $dst  # both local, so a symbolic link suffices
    
    def download(src, dst):
        ln -s $src $dst  # both local, so a symbolic link suffices
        
    def execute(command):
        bash -e $command
</code></pre>
<h4 id="example-define-a-file-system-over-sftp"><a class="header" href="#example-define-a-file-system-over-sftp">Example: define a file system over SFTP</a></h4>
<pre><code class="language-sh">import ssh
object my_server = ssh.server(host="...")
</code></pre>
<h4 id="example-define-a-file-system-over-aws-s3"><a class="header" href="#example-define-a-file-system-over-aws-s3">Example: define a file system over AWS S3</a></h4>
<pre><code class="language-sh">import aws
object my_bucket = aws.s3(name="...")
</code></pre>
<h4 id="example-define-a-file-system-over-azure-blob-storage"><a class="header" href="#example-define-a-file-system-over-azure-blob-storage">Example: define a file system over Azure Blob Storage</a></h4>
<pre><code class="language-sh">import az
object my_container = az.storage_blob(name="...")
</code></pre>
<h3 id="transferring-file-between-environments"><a class="header" href="#transferring-file-between-environments">Transferring file between environments</a></h3>
<p>Sometimes different parts of a pipeline are run under different environments,
e.g., data preprocessing may happen on a local machine, whereas training is done on an SSH grid, or
on AWS EC2 or Azure ML.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="module-std"><a class="header" href="#module-std">Module <code>std</code></a></h1>
<p>Contains some miscellaneous utilities for HyperMake.</p>
<h2 id="function-stdsymlink"><a class="header" href="#function-stdsymlink">Function <code>std.symlink</code></a></h2>
<p>Creates a symbolic link as an output. This is particularly useful when referring to a local repository that is under development.</p>
<pre><code class="language-py">import std
package my_repo = std.symlink(path="path/to/my/repo")
</code></pre>
<h2 id="class-stdrun"><a class="header" href="#class-stdrun">Class <code>std.run</code></a></h2>
<p>Enables a task in HyperMake to run in a custom interpreter (e.g. Python, Perl, etc.).</p>
<p>Example usage:</p>
<pre><code class="language-py">import std

sender = {Sender: Alice Bob}

@std.run(interpreter="python3")
task hello_world(sender=$):
    import os
    print(f"Hello, world from {os.environ["sender"]}!")
</code></pre>
<p>Note that whatever interpreter you choose to use, HyperMake parameters are passed into the task as <strong>environment variables</strong>. Here in Python we use <code>os.environ</code> to access them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module-aws"><a class="header" href="#module-aws">Module <code>aws</code></a></h1>
<h2 id="awss3"><a class="header" href="#awss3"><code>aws.s3</code></a></h2>
<p>Enables AWS S3 buckets as a HyperMake file system. Behind the scenes it uses the <code>aws s3</code> CLI command family.</p>
<p>Example usage:</p>
<pre><code class="language-python">import aws
object my_bucket = aws.s3(
    bucket="my_bucket",
    root=""
)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module-az"><a class="header" href="#module-az">Module <code>az</code></a></h1>
<p>Enables various decorators for Microsoft Azure services in HyperMake.</p>
<h2 id="azstorage_blob"><a class="header" href="#azstorage_blob"><code>az.storage_blob</code></a></h2>
<p>Enables Azure Blob Storage containers to be used as a file system in HyperMake. Behind the scenes it uses the <code>az storage blob</code> CLI command family.</p>
<p>Example usage:</p>
<pre><code class="language-py">import az 
object az_storage = az.storage_blob( 
    container="my_container", 
    extra_args="--account-name xxx --account-key yyy"
)

data_path = "/path/to/data"@az_storage
</code></pre>
<h2 id="azstorage_fs"><a class="header" href="#azstorage_fs"><code>az.storage_fs</code></a></h2>
<p>Enables Azure Data Lake Storage (ADLS) Gen2 containers to be used as a file system in HyperMake. Behind the scenes it uses the <code>az storage fs</code> CLI command family.</p>
<h2 id="azml_job_create"><a class="header" href="#azml_job_create"><code>az.ml_job_create</code></a></h2>
<p>Enables Azure ML command jobs as a submitter in HyperMake. Behind the scenes it uses the <code>az ml job</code> CLI command family.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module-conda"><a class="header" href="#module-conda">Module <code>conda</code></a></h1>
<p>Enables Conda environments to be used as decorators in HyperMake.</p>
<h2 id="function-condacreate_env"><a class="header" href="#function-condacreate_env">Function <code>conda.create_env</code></a></h2>
<p>Creates a Conda environment based on a yaml specification file.</p>
<pre><code class="language-python">package env = conda.create_env(file="environment.yml")
</code></pre>
<h2 id="class-condaactivate"><a class="header" href="#class-condaactivate">Class <code>conda.activate</code></a></h2>
<p>Enables a job to be run within a Conda environment.</p>
<pre><code class="language-python">import conda

@conda.activate(environment="myenv")
task check_if_cuda_is_available():
    python -c "import torch; print(torch.cuda.is_available())"
</code></pre>
<p>You can use the returned path of <code>conda.create_env</code> as the <code>environment</code> argument.</p>
<pre><code class="language-python">package env = conda.create_env(file="environment.yml")
@conda.activate(environment=$env)
</code></pre>
<p>This can even be expressed with nested decorators:</p>
<pre><code class="language-python">import std
import conda

@conda.activate(environment="myenv")
@std.run(interpreter="python")
task check_if_cuda_is_available():
    import torch
    print(torch.cuda.is_available())
</code></pre>
<p>Here we first wrap the script with a <code>python</code> interpreter, then dictate that this task should run within a Conda environment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module-ssh"><a class="header" href="#module-ssh">Module <code>ssh</code></a></h1>
<p>Enables SSH servers to be used as file systems in HyperMake.</p>
<h2 id="sshserver"><a class="header" href="#sshserver"><code>ssh.Server</code></a></h2>
<p>Defines a SSH server in HyperMake. Note that this file system is able to execute jobs.</p>
<p>Example:</p>
<pre><code class="language-py">import ssh
object my_server = ssh.server(
    host='192.168.0.7',    # host name, in ~/.ssh/config
    root='/home/user/out'  # root of HyperMake output on the remote server
)

task my_remote_task@my_server(input@server) -&gt; output@my_server:
    # This task will be executed on the remote server
    # and the input will be copied to the remote server.
    # The output is expected to appear on the remote server.
    ...
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
