{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HyperMake","text":"<p><code>HyperMake</code> is a parameterized workflow manager (think of a <code>make</code> where tasks can be parameterized) heavily inspired by Ducttape.</p> <p>It supports the following features:</p> <ul> <li>Parameterization of tasks: a task can be of multiple versions (e.g. in a ML pipeline, a task can be of different hyperparameters)</li> <li>Minimal juggling: Inputs and outputs are just files/symlinks, and arguments are all passed as environment variables</li> <li>Automatic parallelization: based on the dependency DAG</li> <li>Cloud support: tasks can be easily be decorated run on cloud (e.g. AWS, Azure)</li> </ul>"},{"location":"stdlib/aws/","title":"Module <code>aws</code>","text":""},{"location":"stdlib/aws/#awss3","title":"<code>aws.s3</code>","text":""},{"location":"stdlib/az/","title":"Module <code>az</code>","text":"<p>Enables various decorators for Microsoft Azure services in HyperMake.</p>"},{"location":"stdlib/az/#azstorage_blob","title":"<code>az.storage_blob</code>","text":"<p>Enables Azure Blob Storage containers to be used as a file system in HyperMake. Behind the scenes it uses the <code>az storage blob</code> CLI command family.</p> <p>Example usage: <pre><code>import az \nobject az_storage = az.storage_blob( \n    container=\"my_container\", \n    extra_args=\"--account-name xxx --account-key yyy\"\n)\n\ndata_path = \"/path/to/data\"@az_storage\n</code></pre></p>"},{"location":"stdlib/az/#azstorage_fs","title":"<code>az.storage_fs</code>","text":"<p>Enables Azure Data Lake Storage (ADLS) Gen2 containers to be used as a file system in HyperMake. Behind the scenes it uses the <code>az storage fs</code> CLI command family.</p>"},{"location":"stdlib/az/#azml_job_create","title":"<code>az.ml_job_create</code>","text":"<p>Enables Azure ML command jobs as a submitter in HyperMake. Behind the scenes it uses the <code>az ml job</code> CLI command family.</p>"},{"location":"stdlib/conda/","title":"Module <code>conda</code>","text":"<p>Enables Conda environments to be used as decorators in HyperMake.</p>"},{"location":"stdlib/conda/#function-condacreate_env","title":"Function <code>conda.create_env</code>","text":"<p>Creates a Conda environment based on a yaml specification file.</p> <pre><code>package env = conda.create_env(file=\"environment.yml\")\n</code></pre>"},{"location":"stdlib/conda/#class-condaactivate","title":"Class <code>conda.activate</code>","text":"<p>Enables a job to be run within a Conda environment.</p> <pre><code>import conda\n\n@conda.activate(environment=\"myenv\")\ntask check_if_cuda_is_available():\n    python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>You can use the returned path of <code>conda.create_env</code> as the <code>environment</code> argument.</p> <pre><code>package env = conda.create_env(file=\"environment.yml\")\n@conda.activate(environment=$env)\n</code></pre> <p>This can even be expressed with nested decorators: <pre><code>import std\nimport conda\n\n@conda.activate(environment=\"myenv\")\n@std.run(interpreter=\"python\")\ntask check_if_cuda_is_available():\n    import torch\n    print(torch.cuda.is_available())\n</code></pre> Here we first wrap the script with a <code>python</code> interpreter, then dictate that this task should run within a Conda environment.</p>"},{"location":"stdlib/ssh/","title":"Module <code>ssh</code>","text":"<p>Enables SSH servers to be used as file systems in HyperMake.</p>"},{"location":"stdlib/ssh/#sshserver","title":"<code>ssh.Server</code>","text":"<p>Defines a SSH server in HyperMake. Note that this file system is able to execute jobs.</p> <p>Example: <pre><code>import ssh\nobject my_server = ssh.server(\n    host='192.168.0.7',    # host name, in ~/.ssh/config\n    root='/home/user/out'  # root of HyperMake output on the remote server\n)\n\ntask my_remote_task@my_server(input@server) -&gt; output@my_server:\n    # This task will be executed on the remote server\n    # and the input will be copied to the remote server.\n    # The output is expected to appear on the remote server.\n    ...\n</code></pre></p>"},{"location":"stdlib/std/","title":"Module <code>std</code>","text":"<p>Contains some miscellaneous utilities for HyperMake.</p>"},{"location":"stdlib/std/#function-stdsymlink","title":"Function <code>std.symlink</code>","text":"<p>Creates a symbolic link as an output. This is particularly useful when referring to a local repository that is under development. <pre><code>import std\npackage my_repo = std.symlink(path=\"path/to/my/repo\")\n</code></pre></p>"},{"location":"stdlib/std/#class-stdrun","title":"Class <code>std.run</code>","text":"<p>Enables a task in HyperMake to run in a custom interpreter (e.g. Python, Perl, etc.).</p> <p>Example usage: <pre><code>import std\n\nsender = {Sender: Alice Bob}\n\n@std.run(interpreter=\"python3\")\ntask hello_world(sender=$):\n    import os\n    print(f\"Hello, world from {os.environ[\"sender\"]}!\")\n</code></pre></p> <p>Note that whatever interpreter you choose to use, HyperMake parameters are passed into the task as environment variables. Here in Python we use <code>os.environ</code> to access them.</p>"},{"location":"tutorial/branch/","title":"Parameters","text":"<p>We'd like to parameterize our tasks: there could be multiple versions of a task. This could be different datasets, different preprocessing pipelines, different pretrained models, etc. Hypermake models these configurations as parameters to values and tasks.</p> <p>For example, we are here downloading various versions of the GloVe embeddings from the Stanford NLP website:</p> <pre><code>gloveUrl = {Version:\n  6b=\"http://nlp.stanford.edu/data/glove.6B.zip\"\n  cc42b=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n  cc840b=\"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n  twitter27b=\"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n}\n\ntask downloadGloVe(gloveUrl=$) -&gt; out:\n  sleep $((( $RANDOM % 10 ) + 1))s\n  wget -O glove.zip $gloveUrl\n  unzip glove.zip\n  mv *.txt $out\n  rm glove.zip\n</code></pre> <p>If the key is the same as the value, one can omit the value in the declaration, like this: <code>{Version: 6b cc42b cc840b twitter27b}</code>.</p> <p>Note that we declared a <code>Dict</code>-like object here: <code>{Version: 6b=XX cc42b=XX cc840b=XX twitter27b=XX}</code>. This is a parameter declaration: the name of the parameter is <code>Version</code>, and it has 4 potential keys <code>6b</code>, <code>cc42b</code>, <code>cc840b</code>, and <code>twitter27b</code>. Each key is associated with a value that follows the key (that is the URL in this example).</p> <p>Our task <code>downloadGloVe</code> is now parameterized with variable <code>Version</code>. To refer to one of these task (a case), we can use the indexing notation <code>task[Var:key]</code>: here for example <code>downloadGloVe[Version:6b]</code>.</p> <p>We can use the task referring expressions in the Hypermake command line interface. To download GloVe <code>6b</code> version, do <pre><code>hypermake glove.hm run downloadGloVe[Version:6b]\n</code></pre> One can use <code>[Var:*]</code> syntax to refer to all cases of the task. The following command line invocation downloads all GloVe versions: <pre><code>hypermake glove.hm run 'downloadGloVe[Version: *]' -j4\n</code></pre></p> <p>Single-quoting the task <code>downloadGlove[Version: *]</code> prohibits bash from expanding the wildcard <code>*</code> symbol.</p> <p>Again, we can find the output files in <code>out/downloadGloVe/Version=XX</code>.</p> <p><code>-j4</code> is a flag that specifies the number of parallel jobs to run. Here we run 4 jobs in parallel.</p>"},{"location":"tutorial/compose/","title":"Inputs, outputs, and composing tasks","text":"<p>A task can take parameters, and yield outputs. <pre><code>url = \"https://news.ycombinator.org\"\ntask download(url=$) -&gt; (out=\"homepage.html\"):\n  wget $url -O $out\n</code></pre></p> <p>Generally, when declaring a parameter whose default argument is a variable with the same name, one can omit the argument name by just writing <code>$</code>.</p> <p>Running the task <code>download</code> will download the homepage of the Hacker News. Note the parameter is declared as <code>url=$</code>:  This is a shorthand for <code>url=$url</code>.</p> <p>This task creates a single output called <code>homepage.html</code>: You can find this file at <code>out/download/default</code> directory.  One can simply write <code>out</code> instead of <code>(out=\"homepage.html\")</code>: in this case the output file name will be <code>out</code>.</p> <p>Now, we'd like to extract all the headlines: their URLs and titles. We create another task that takes the output of the <code>download</code> task as input: <pre><code>task get_titles(html=$download.out) -&gt; out:\n  cat $html \\\n  | perl -ne 'if (/&lt;a href=\"(.*?)\" class=\"storylink\"&gt;(.*?)&lt;\\/a&gt;/) { print \"$1\\t$2\\n\" }' \\\n  &gt; $out\n</code></pre></p> <p>Note that to refer to the output of a task, one can use the syntax <code>$taskName.$outputName</code>. Here <code>$download.out</code> is the <code>out</code> output of the <code>download</code> task.</p> <p>It creates the following pipeline:</p> <pre><code>flowchart LR\n    download --&gt; get_titles</code></pre> <p>Running the following command <pre><code>hypermake tutorial/ycomb.hm run get_titles\n</code></pre> will sequentially run the two dependent jobs: first <code>download</code> then <code>get_titles</code>, and the resulting TSV table will be located in <code>out/get_titles/default/out</code>.</p> <p>In the directory <code>out/get_titles/default</code>, you can find the following files that may be of interest:</p> <ul> <li><code>out</code>: the resulting TSV table</li> <li><code>script</code>: the script that was used to run the task</li> <li><code>stdout</code>: the standard output of the task</li> <li><code>stderr</code>: the standard error of the task</li> <li><code>args</code>: A shell script that contains the arguments that were used to run the task</li> <li><code>exitcode</code>: A file that contains the exit code of the task</li> </ul> <p>To recreate the task in shell, you can use the following command. It should behave the same as if directly running from HyperMake. <pre><code>. args &amp;&amp; . script\n</code></pre></p>"},{"location":"tutorial/decorators/","title":"Decorators","text":"<p>In Hypermake, a task can be decorated with some decorators, effectively modifying its behavior. This can support </p> <ul> <li>Running with different shell;</li> <li>Running in specific virtual environments;</li> <li>Running through some cluster submission systems;</li> <li>etc.</li> </ul> <p>The general syntax for defining a decorator is:</p> <p><pre><code>def &lt;decoratorName&gt;(&lt;arguments&gt;) &lt;- input=&lt;internalScriptName&gt;:\n  &lt;script&gt;\n</code></pre> and when applying a decorator, one could write <pre><code>@&lt;decoratorName&gt;(&lt;arguments&gt;)\ntask taskName(...) -&gt; out:\n  ...\n</code></pre></p> <p>This will modify the task with the decorator. The script to be modified will be stored as <code>internalScriptName</code>.</p>"},{"location":"tutorial/decorators/#example-1-a-simple-decorator","title":"Example 1: A simple decorator","text":"<p>An example that let us runs a task in Python instead of shell: <pre><code>def python() &lt;- internalPythonScript=\"script.py\":\n  python $internalPythonScript\n\n@python\ntask helloWorldInPython:\n  print(\"Hello World\" + \" \" + \"in Python!\")\n</code></pre></p> <p>And we can invoke the task from the command line: <pre><code>hypermake tutorial/decorators.hm run helloWorldInPython\n</code></pre></p> <p>Let's see what's happening under the hood here. The script in the task <code>helloWorldInPython</code> is decorated with <code>@python</code>. The script with the line <code>print(...)</code> is stored as <code>script.py</code> as directed in the decorator. Then, in shell, the command in the decorator is run instead: <code>python script.py</code>.</p>"},{"location":"tutorial/decorators/#example-2-a-parameterized-decorator","title":"Example 2: A parameterized decorator","text":"<p>In Python, a task can be run in different Conda virtual environments. This is a decorator that lets us do that.</p> <pre><code>def conda(env) &lt;- internalCondaScript=\"conda-internal.sh\":\n  eval \"$(command conda 'shell.bash' 'hook' 2&gt; /dev/null)\"\n  conda activate $env\n  . $internalCondaScript\n  conda deactivate\n\n@conda(env={Env: base myenv})\ntask helloWorldFromEnv:\n  python -c \"print('Hello World in Python from $env!')\"\n</code></pre> <p>Note that in the task <code>helloWorldFromEnv</code>, the decorator <code>conda</code> has a parameterized argument: <code>env={Env: base myenv}</code>. We can invoke both cases of the task <code>helloWorldFromEnv</code>: <pre><code>hypermake tutorial/decorators.hm run 'helloWorldFromEnv[Env: *]'\n</code></pre></p> <p>We will see both lines <pre><code>Hello World in Python from base!\nHello World in Python from myenv!\n</code></pre> output to the terminal.</p>"},{"location":"tutorial/decorators/#example-3-chaining-decorators","title":"Example 3: Chaining decorators","text":"<p>We have now created two decorators:</p> <ul> <li><code>@python</code> that executes a script using Python instead of Bash as the interpreter;</li> <li><code>@conda</code> that runs a task in a specific Conda virtual environment.</li> </ul> <p>Can we compose these decorators? Yes.</p> <pre><code>@conda(env={Env: base myenv})\n@python\ntask helloWorldInPythonFromEnv:\n  import os\n  print(f\"Hello World in Python from {os.environ['env']}!\")\n</code></pre> <p>One can use <code>os.environ[var]</code> to get the environment variable <code>$var</code> in Python.</p> <p>First, our script is wrapped by <code>@python</code>, then <code>@conda(env)</code>.  Recall that Hypermake passes parameters into the script as environment variables:  we cannot use <code>$env</code> to get the Hypermake variable in Python.</p>"},{"location":"tutorial/example-beir/","title":"BEIR","text":"<p>We showcase an example pipeline of running the BEIR (paper) benchmark in HyperMake.</p> <p>BEIR is a robust and heterogeneous evaluation benchmark for zero-shot information retrieval. It includes a diverse set of retrieval tasks, such as web search, question answering, and entity retrieval. The benchmark is designed to evaluate the generalization capabilities of retrieval models across different tasks and domains.</p> <p>In this example, we run the standard BM25 lexical retrieval model (with Pyserini) on the BEIR-14 subset (those with public licenses), evaluated with the standard <code>trec_eval</code> evaluation tool.</p> <p>First, let's import some modules to simplify the pipeline definition: <pre><code>import conda\nimport std\n</code></pre></p> <p>Next, we define some hyperparameters for the pipeline. </p> <p>BEIR-14 datasets, defined as a HyperMake variable: <pre><code>beir_dataset = {BeirDataset:\n  msmarco trec-covid nfcorpus nq hotpotqa fiqa\n  arguana webis-touche2020 quora dbpedia-entity\n  scidocs fever climate-fever scifact\n}\n</code></pre> Partition of the datasets that we want to evaluate (MSMARCO is evaluated on the <code>dev</code> set), defined as a HyperMake dict: <pre><code>test_partition = {BeirDataset:\n  msmarco=dev trec-covid=test nfcorpus=test nq=test hotpotqa=test fiqa=test\n  arguana=test webis-touche2020=test quora=test dbpedia-entity=test\n  scidocs=test fever=test climate-fever=test scifact=test\n}\n</code></pre></p> <p>And a constant for the data downloading URL prefix: <pre><code>beir_url_prefix = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets\"\n</code></pre></p>"},{"location":"tutorial/example-beir/#step-1-build-the-trec_eval-tool","title":"Step 1: Build the <code>trec_eval</code> tool","text":"<pre><code>package trec_eval -&gt; out:\n  git clone https://github.com/usnistgov/trec_eval.git $out\n  cd $out\n  make\n</code></pre>"},{"location":"tutorial/example-beir/#step-2-build-our-conda-environment-for-pyserini","title":"Step 2: Build our Conda environment for Pyserini","text":"<p><pre><code>package pyserini = conda.create(\n  args=\"-c conda-forge python=3.10 openjdk=21\",\n  extra_pip_packages=\"torch faiss-cpu pyserini\"\n)\n</code></pre> Note that both of these steps are packages instead of <code>tasks</code>: these are platform-dependent and should be built for each platform separately.</p>"},{"location":"tutorial/example-beir/#step-3-download-the-beir-datasets","title":"Step 3: Download the BEIR datasets","text":"<pre><code>task raw_beir_data(beir_dataset=$, beir_url_prefix=$) -&gt; out:\n  wget -O dataset.zip $beir_url_prefix/$beir_dataset.zip\n  unzip dataset.zip\n  rm dataset.zip\n  mv $beir_dataset out\n</code></pre>"},{"location":"tutorial/example-beir/#step-4-convert-the-beir-datasets-to-standard-trec-format","title":"Step 4: Convert the BEIR datasets to standard TREC format","text":"<p>Here Python code is embedded directly in the task definition with <code>std.run</code> as an example, showing how to use Python code directly in the task definition.  This is useful for small scripts that are not worth putting in a separate file.  This script is actually large enough that it would be better to put it in a separate file, but for the sake of demonstration, we include it here.</p> <p>Note the use of <code>@conda.activate</code> to run this task in the Pyserini environment we created earlier. <pre><code>@conda.activate(environment=$pyserini)\n@std.run(interpreter=\"python\")\ntask beir_to_trec(data=$raw_beir_data.out) -&gt; out:\n  import os\n  import json\n  import sys\n  import csv\n  from tqdm import tqdm\n  data = os.environ['data']  # HyperMake variables are sent as environment variables\n  out = os.environ['out']\n  os.mkdir(out)\n  with open(f\"{data}/corpus.jsonl\") as f_in, open(f\"{out}/corpus\", 'w') as f_out:\n    for line in tqdm(f_in):\n      obj = json.loads(line)\n      id = obj['_id']\n      text = obj['text'].replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n      title = obj.get('title', \"\")\n      trec_line = f\"{id}\\t{title}: {text}\" if title != \"\" else f\"{id}\\t{text}\" \n      # Concatenate title and text\n      print(trec_line, file=f_out)\n  queries = {}\n  with open(f\"{data}/queries.jsonl\") as f:\n    for line in tqdm(f):\n      obj = json.loads(line)\n      id = obj['_id']\n      text = obj['text']\n      queries[id] = text\n  for partition in os.listdir(f\"{data}/qrels\"):\n    partition = os.path.splitext(partition)[0]\n    with open(f\"{data}/qrels/{partition}.tsv\") as f_in, open(f\"{out}/{partition}.qrels\", 'w') as f_out:\n      query_ids = set()\n      for row in tqdm(csv.DictReader(f_in, delimiter='\\t')):\n        query_ids.add(row['query-id'])\n        print(f\"{row['query-id']}\\t0\\t{row['corpus-id']}\\t{row['score']}\", file=f_out)\n    with open(f\"{out}/{partition}.queries\", 'w') as f:\n      for query_id in query_ids:\n        print(f\"{query_id}\\t{queries[query_id]}\", file=f)\n</code></pre></p>"},{"location":"tutorial/example-beir/#step-5-index-the-corpus-with-pyserini","title":"Step 5: Index the corpus with Pyserini","text":"<p>At this step, we run a Bash script under the Pyserini conda environment. <pre><code>@conda.activate(environment=$pyserini)\ntask index(data=$beir_to_trec.out) -&gt; out:\n  mkdir corpus\n  cat $data/corpus \\\n    | jq -Rc 'inputs | split(\"\\t\") | {id: .[0], contents: .[1]}' \\\n    &gt; corpus/corpus.json  # Convert TREC format to Pyserini JSON\n  python -m pyserini.index.lucene \\\n    --collection JsonCollection \\\n    --input corpus \\\n    --index $out \\\n    --generator DefaultLuceneDocumentGenerator \\\n    --threads $(nproc) \\\n    --storePositions \\\n    --storeDocvectors \\\n    --storeRaw\n</code></pre></p>"},{"location":"tutorial/example-beir/#step-6-run-bm25-retrieval-with-pyserini","title":"Step 6: Run BM25 retrieval with Pyserini","text":"<p>Again, this is run under the Pyserini conda environment. <pre><code>@conda.activate(environment=$pyserini)\ntask retrieve(\n  data=$beir_to_trec.out, \n  test_partition=$, \n  index=$index.out\n) -&gt; (out=\"result.qres\"):\n  ln -s $data/$test_partition.queries test.tsv\n  python -m pyserini.search.lucene \\\n    --index $index \\\n    --topics test.tsv \\\n    --output $out \\\n    --batch-size 32 \\\n    --hits 100 \\\n    --threads $(nproc) \\\n    --remove-duplicates --remove-query --bm25\n</code></pre></p>"},{"location":"tutorial/example-beir/#step-7-evaluate-the-retrieval-results-with-trec_eval","title":"Step 7: Evaluate the retrieval results with <code>trec_eval</code>","text":"<pre><code>task evaluate(\n  data=$beir_to_trec.out,\n  result=$retrieve.out,\n  test_partition=$,\n  trec_eval=$\n) -&gt; (out=\"eval.txt\"):\n  $trec_eval/trec_eval -m all_trec $data/$test_partition.qrels $result &gt; $out\n</code></pre>"},{"location":"tutorial/example-beir/#step-8-aggregate-the-evaluation-results-over-all-datasets-in-beir-14","title":"Step 8: Aggregate the evaluation results over all datasets in BEIR-14","text":"<p>Assume that we care about <code>map</code>, <code>recall_100</code>, and <code>ndcg_cut_10</code> metrics from <code>trec_eval</code>. <pre><code>metric = {Metric: map recall_100 ndcg_cut_10}\n</code></pre> And now we aggregate the results over all datasets with <code>[BeirDataset: *]</code> in HyperMake: <pre><code>task aggregate_metric(\n  eval_results=$evaluate[BeirDataset: *].out, \n  metric=$\n) -&gt; (out=\"aggregate.txt\"):\n  grep -E \"^$metric \" $eval_results/* &gt; $out\n</code></pre></p>"},{"location":"tutorial/example-beir/#run-the-pipeline","title":"Run the pipeline","text":"<p>Now the full pipeline definition (<code>beir.hm</code>) is complete.  We can run the pipeline with the following command: <pre><code>hypermake beir.hm run \"aggregate_metric[Metric: *]\" -j8\n</code></pre> Here we compute the <code>aggregate_metric</code> task for all metrics defined in <code>metric</code>, with max 8 jobs running in parallel!</p>"},{"location":"tutorial/filesys/","title":"File systems","text":"<p>A file system encapsulates the operations that can be performed on files and directories in a particular environment in HyperMake.</p> <p>HyperMake provides a default file system implementation for the local file system (<code>local</code>),  and has utilities to define file systems over common remote systems such as SFTP, AWS S3, and Azure Blob Storage. </p> <p>Additionally, it is possible to define custom file systems for different environments.</p> <p>In HyperMake, a file system is an object with various member functions defined.</p>"},{"location":"tutorial/filesys/#functions-in-a-file-system-object","title":"Functions in a file system object","text":"Member Description <code>fs.root</code> A string specifying the root path of all HyperMake outputs. <code>fs.read(file)</code> Reads the file <code>$file</code> and outputs the content to <code>stdout</code>. <code>fs.mkdir(dir)</code> Creates an empty directory <code>$dir</code>.  This should have the semantics of <code>mkdir -p</code>: it should create all parent  directories if they do not exist, and it should not fail if the directory  already exists. <code>fs.exists(file)</code> Checks if <code>$file</code> exists in <code>fs</code>. <code>fs.link(src, dst)</code> Creates a symbolic link at <code>$dst</code> that links to <code>$src</code>. <code>fs.touch(file)</code> Creates an empty file at path `$file <code>fs.remove(file)</code> Removes file <code>$file</code> in <code>fs</code>.  If <code>$file</code> is a directory, it should remove the directory and all its contents. <code>fs.upload(src, dst)</code> Uploads the file or directory <code>$src</code> in <code>local</code> to <code>$dst</code> in <code>fs</code>. <code>fs.download(src, dst)</code> Downloads the file or directory <code>$src</code> in <code>fs</code> to <code>$dst</code> in <code>local</code>. <code>fs.execute(command)</code> (Optional) Executes the command <code>$command</code> in <code>fs</code>'s shell.  This can be omitted if the file system does not support running commands. <p>There is no need to define <code>local</code> as it is internal to HyperMake. A reference implementation of <code>local</code> is provided below. <pre><code>object local:\n    root = \".\"\n\n    def read(file):\n        cat $file\n\n    def mkdir(dir):\n        mkdir -p $dir\n\n    def exists(file):\n        test -e $file\n\n    def link(src, dst):\n        ln -s $src $dst\n\n    def touch(file):\n        touch $file\n\n    def remove(file):\n        rm -r $file\n\n    def upload(src, dst):\n        ln -s $src $dst  # both local, so a symbolic link suffices\n\n    def download(src, dst):\n        ln -s $src $dst  # both local, so a symbolic link suffices\n\n    def execute(command):\n        bash -e $command\n</code></pre></p>"},{"location":"tutorial/filesys/#example-define-a-file-system-over-sftp","title":"Example: define a file system over SFTP","text":"<pre><code>import ssh\nobject my_server = ssh.server(host=\"...\")\n</code></pre>"},{"location":"tutorial/filesys/#example-define-a-file-system-over-aws-s3","title":"Example: define a file system over AWS S3","text":"<pre><code>import aws\nobject my_bucket = aws.s3(name=\"...\")\n</code></pre>"},{"location":"tutorial/filesys/#example-define-a-file-system-over-azure-blob-storage","title":"Example: define a file system over Azure Blob Storage","text":"<pre><code>import az\nobject my_container = az.storage_blob(name=\"...\")\n</code></pre>"},{"location":"tutorial/filesys/#transferring-file-between-environments","title":"Transferring file between environments","text":"<p>Sometimes different parts of a pipeline are run under different environments,  e.g., data preprocessing may happen on a local machine, whereas training is done on an SSH grid, or  on AWS EC2 or Azure ML. </p>"},{"location":"tutorial/hello/","title":"Runs a simple \"Hello, world\" task","text":"<p>First let's define our first task (without any inputs or outputs declared!):</p> <p>Note the syntax here: A code block starts after the <code>:</code> at the end of the task signature. A code block is a consecutive list of indented lines of scripts, where each line must start with at least 2 spaces.</p> <pre><code>task hello:\n  echo \"Hello, world!\"\n</code></pre> <p>Save this file as <code>hello.hm</code>. </p> <p>We have created our first HyperMake script file that contains a single task. Now let's run this task!</p> <p>Execute the following command in your shell:</p> <p>The basic command line usage is <code>hypermake $script $subtask $target</code>. Here the <code>$subtask</code> is simply <code>run</code>.</p> <p><pre><code>  hypermake hello.hm run hello \n</code></pre> We should see the output \"Hello, world!\" printed in the terminal.</p>"},{"location":"tutorial/modules/","title":"Classes/Objects","text":"<p>In Hypermake, sometimes it is needed to bundle certain definitions together so that it can be reused.  This forms an <code>object</code> (or modules, since a module can be seen as a singleton object):</p> <pre><code>object my_obj:\n  def x(...):\n  def y(...):\n</code></pre>"},{"location":"tutorial/packages/","title":"Packages","text":"<p>In Hypermake, packages are special tasks that builds a software package.  They can depends on other packages but not tasks, and will be built differently on different environments (see next tutorial).</p> <p>A package is defined as follows: <pre><code>package $packageName -&gt; $packageOutputName:\n  # build script\n</code></pre> For example, let's build <code>trec_eval</code> (a standard information retrieval evaluation toolkit from NIST) from its C source code: <pre><code>package trec_eval -&gt; out:\n  mkdir -p $out\n  git clone https://github.com/usnistgov/trec_eval $out\n  cd $out\n  make\n</code></pre> Here we clone the repository into a HyperMake-managed directory <code>$out</code>, and then run <code>make</code> to build the package. The binary will be built in <code>$out</code>.</p> <p>To refer to this  package output, use <code>$trec_eval</code> (there is no need to specify <code>$trec_eval.out</code>).  For example, if an evaluation task requires this package, one can write <pre><code>task eval(trec_eval=$, pred=$, gold=$) -&gt; out:\n  $trec_eval/trec_eval $gold $pred &gt; $out\n</code></pre></p>"},{"location":"tutorial/packages/#example-1-copying-a-package-from-a-local-directory","title":"Example 1: Copying a package from a local directory","text":"<p><pre><code>package pack1 -&gt; out:\n  ln -s $localDir $out\n</code></pre> This behavior can be written as  <pre><code>import std\npackage pack1 = std.symlink(path=$localDir)\n</code></pre></p>"},{"location":"tutorial/packages/#example-2-cloning-from-a-remote-repository","title":"Example 2: Cloning from a remote repository","text":"<pre><code>package pack2 -&gt; out:\n  git clone $repo out\n</code></pre>"},{"location":"tutorial/packages/#example-3-call-its-makefile-after-cloning-from-a-repo","title":"Example 3: Call its Makefile after cloning from a repo","text":"<pre><code>package pack3(repo=$) -&gt; out:\n  git clone $repo out\n  cd out\n  make\n</code></pre>"},{"location":"tutorial/packages/#example-4-creates-a-conda-environment-from-a-python-package","title":"Example 4: Creates a Conda environment from a Python package","text":"<pre><code>package pack4(pythonPackage=$) -&gt; out:\n  mkdir -p $out\n  conda env create -p $out -f $pythonPackage/environment.yml\n</code></pre>"},{"location":"tutorial/parameterized-compose/","title":"Composing parameterized tasks","text":""}]}