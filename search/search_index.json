{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"stdlib/azure/","title":"Module <code>azure</code>","text":""},{"location":"stdlib/azure/#azureblobstorage","title":"<code>azure.BlobStorage</code>","text":"<p>Enables Azure Blob Storage support.</p> <p>Example usage: <pre><code>import azure \nobject az_storage = azure.BlobStorage(\n    name = \"az_storage\", \n    container = \"my_container\", \n    extra_args = \"--account-name xxx --account-key yyy\"\n)\n\ndata_path = \"/path/to/data\" @az_storage\n</code></pre></p>"},{"location":"stdlib/azure/#azuredatalakestoragegen2","title":"<code>azure.DataLakeStorageGen2</code>","text":""},{"location":"stdlib/azure/#azuremlcompute","title":"<code>azure.MLCompute</code>","text":""},{"location":"stdlib/conda/","title":"Module <code>conda</code>","text":""},{"location":"stdlib/conda/#condaenvironment","title":"<code>conda.Environment</code>","text":"<p>Enables a job runs within a Conda environment.</p> <pre><code>import conda\n\nobject env = conda.Environment(name=\"myenv\")\n\n@env\ntask check_if_cuda_is_available():\n  python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"stdlib/sftp/","title":"Module <code>sftp</code>","text":""},{"location":"stdlib/sftp/#sftpserver","title":"<code>sftp.Server</code>","text":""},{"location":"tutorial/branch/","title":"Parameters","text":"<p>We'd like to parameterize our tasks: there could be multiple versions of a task. This could be different datasets, different preprocessing pipelines, different pretrained models, etc. Hypermake models these configurations as parameters to values and tasks.</p> <p>For example, we are here downloading various versions of the GloVe embeddings from the Stanford NLP website:</p> <pre><code>gloveUrl = {Version:\n  6b=\"http://nlp.stanford.edu/data/glove.6B.zip\"\n  cc42b=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n  cc840b=\"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n  twitter27b=\"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n}\n\ntask downloadGloVe(gloveUrl=$) -&gt; out:\n  sleep $((( $RANDOM % 10 ) + 1))s\n  wget -O glove.zip $gloveUrl\n  unzip glove.zip\n  mv *.txt $out\n  rm glove.zip\n</code></pre> <p>If the key is the same as the value, one can omit the value in the declaration, like this: <code>{Version: 6b cc42b cc840b twitter27b}</code>.</p> <p>Note that we declared a <code>Dict</code>-like object here: <code>{Version: 6b=XX cc42b=XX cc840b=XX twitter27b=XX}</code>. This is a parameter declaration: the name of the parameter is <code>Version</code>, and it has 4 potential keys <code>6b</code>, <code>cc42b</code>, <code>cc840b</code>, and <code>twitter27b</code>. Each key is associated with a value that follows the key (that is the URL in this example).</p> <p>Our task <code>downloadGloVe</code> is now parameterized with variable <code>Version</code>. To refer to one of these task (a case), we can use the indexing notation <code>task[Var:key]</code>: here for example <code>downloadGloVe[Version:6b]</code>.</p> <p>We can use the task referring expressions in the Hypermake command line interface. To download GloVe <code>6b</code> version, do <pre><code>hypermake glove.hm run downloadGloVe[Version:6b]\n</code></pre> One can use <code>[Var:*]</code> syntax to refer to all cases of the task. The following command line invocation downloads all GloVe versions: <pre><code>hypermake glove.hm run 'downloadGloVe[Version: *]' -j4\n</code></pre></p> <p>Single-quoting the task <code>downloadGlove[Version: *]</code> prohibits bash from expanding the wildcard <code>*</code> symbol.</p> <p>Again, we can find the output files in <code>out/downloadGloVe/Version=XX</code>.</p> <p><code>-j4</code> is a flag that specifies the number of parallel jobs to run. Here we run 4 jobs in parallel.</p>"},{"location":"tutorial/compose/","title":"Inputs, outputs, and composing tasks","text":"<p>A task can take parameters, and yield outputs. <pre><code>url = \"https://news.ycombinator.org\"\ntask download(url=$) -&gt; (out=\"homepage.html\"):\n  wget $url -O $out\n</code></pre></p> <p>Generally, when declaring a parameter whose default argument is a variable with the same name, one can omit the argument name by just writing <code>$</code>.</p> <p>Running the task <code>download</code> will download the homepage of the Hacker News. Note the parameter is declared as <code>url=$</code>:  This is a shorthand for <code>url=$url</code>.</p> <p>This task creates a single output called <code>homepage.html</code>: You can find this file at <code>out/download/default</code> directory.  One can simply write <code>out</code> instead of <code>(out=\"homepage.html\")</code>: in this case the output file name will be <code>out</code>.</p> <p>Now, we'd like to extract all the headlines: their URLs and titles. We create another task that takes the output of the <code>download</code> task as input: <pre><code>task get_titles(html=$download.out) -&gt; out:\n  cat $html \\\n  | perl -ne 'if (/&lt;a href=\"(.*?)\" class=\"storylink\"&gt;(.*?)&lt;\\/a&gt;/) { print \"$1\\t$2\\n\" }' \\\n  &gt; $out\n</code></pre></p> <p>It creates the following pipeline:</p> <pre><code>flowchart LR\n    download --&gt; get_titles</code></pre> <p>Running the following command <pre><code>hypermake tutorial/ycomb.hm run get_titles\n</code></pre> will sequentially run the two dependent jobs: first <code>download</code> then <code>get_titles</code>, and the resulting TSV table will be located in <code>out/get_titles/default/out</code>.</p> <p>In the directory <code>out/get_titles/default</code>, you can find the following files that may be of interest:</p> <ul> <li><code>out</code>: the resulting TSV table</li> <li><code>script.sh</code>: the script that was used to run the task</li> <li><code>stdout</code>: the standard output of the task</li> <li><code>stderr</code>: the standard error of the task</li> <li><code>args</code>: A shell script that contains the arguments that were used to run the task</li> <li><code>exitcode</code>: A file that contains the exit code of the task</li> </ul> <p>To recreate the task in shell, you can use the following command. It should behaves the same as if directly running from Hypermake. <pre><code>. args &amp;&amp; . script.sh\n</code></pre></p>"},{"location":"tutorial/decorators/","title":"Decorators","text":"<p>In Hypermake, a task can be decorated with some decorators, effectively modifying its behavior. This can support </p> <ul> <li>Running with different shell;</li> <li>Running in specific virtual environments;</li> <li>Running through some cluster submission systems;</li> <li>etc.</li> </ul> <p>The general syntax for defining a decorator is:</p> <p><pre><code>def &lt;decoratorName&gt;(&lt;arguments&gt;) &lt;- input=&lt;internalScriptName&gt;:\n  &lt;script&gt;\n</code></pre> and when applying a decorator, one could write <pre><code>@&lt;decoratorName&gt;(&lt;arguments&gt;)\ntask taskName(...) -&gt; out:\n  ...\n</code></pre></p> <p>This will modify the task with the decorator. The script to be modified will be stored as <code>internalScriptName</code>.</p>"},{"location":"tutorial/decorators/#example-1-a-simple-decorator","title":"Example 1: A simple decorator","text":"<p>An example that let us runs a task in Python instead of shell: <pre><code>def python() &lt;- internalPythonScript=\"script.py\":\n  python $internalPythonScript\n\n@python\ntask helloWorldInPython:\n  print(\"Hello World\" + \" \" + \"in Python!\")\n</code></pre></p> <p>And we can invoke the task from the command line: <pre><code>hypermake tutorial/decorators.hm run helloWorldInPython\n</code></pre></p> <p>Let's see what's happening under the hood here. The script in the task <code>helloWorldInPython</code> is decorated with <code>@python</code>. The script with the line <code>print(...)</code> is stored as <code>script.py</code> as directed in the decorator. Then, in shell, the command in the decorator is run instead: <code>python script.py</code>.</p>"},{"location":"tutorial/decorators/#example-2-a-parameterized-decorator","title":"Example 2: A parameterized decorator","text":"<p>In Python, a task can be run in different Conda virtual environments. This is a decorator that lets us do that.</p> <pre><code>def conda(env) &lt;- internalCondaScript=\"conda-internal.sh\":\n  eval \"$(command conda 'shell.bash' 'hook' 2&gt; /dev/null)\"\n  conda activate $env\n  . $internalCondaScript\n  conda deactivate\n\n@conda(env={Env: base myenv})\ntask helloWorldFromEnv:\n  python -c \"print('Hello World in Python from $env!')\"\n</code></pre> <p>Note that in the task <code>helloWorldFromEnv</code>, the decorator <code>conda</code> has a parameterized argument: <code>env={Env: base myenv}</code>. We can invoke both cases of the task <code>helloWorldFromEnv</code>: <pre><code>hypermake tutorial/decorators.hm run 'helloWorldFromEnv[Env: *]'\n</code></pre></p> <p>We will see both lines <pre><code>Hello World in Python from base!\nHello World in Python from myenv!\n</code></pre> output to the terminal.</p>"},{"location":"tutorial/decorators/#example-3-chaining-decorators","title":"Example 3: Chaining decorators","text":"<p>We have now created two decorators:</p> <ul> <li><code>@python</code> that executes a script using Python instead of Bash as the interpreter;</li> <li><code>@conda</code> that runs a task in a specific Conda virtual environment.</li> </ul> <p>Can we compose these decorators? Yes.</p> <pre><code>@conda(env={Env: base myenv})\n@python\ntask helloWorldInPythonFromEnv:\n  import os\n  print(f\"Hello World in Python from {os.environ['env']}!\")\n</code></pre> <p>One can use <code>os.environ[var]</code> to get the environment variable <code>$var</code> in Python.</p> <p>First, our script is wrapped by <code>@python</code>, then <code>@conda(env)</code>.  Recall that Hypermake passes parameters into the script as environment variables:  we cannot use <code>$env</code> to get the Hypermake variable in Python.</p>"},{"location":"tutorial/filesys/","title":"File systems","text":""},{"location":"tutorial/filesys/#functions-to-define-for-a-new-environment-env","title":"Functions to define for a new environment <code>$env</code>","text":"Function to define Description Reference implementation in <code>local</code> <code>$env_read(file)</code> Reads the file <code>$file</code> and outputs the content to <code>stdout</code>. <pre>def local_read(file):  cat $file</pre> <code>$env_execute(command)</code> Executes the command <code>$command</code> in <code>$env</code>'s shell. <pre>def local_execute(command):  bash -e $command</pre> <code>$env_mkdir(dir)</code> Creates an empty directory <code>$dir</code>. <pre>def local_mkdir(dir):  mkdir -p $dirpre&gt;\n\n\n<code>$env_exists(file)</code>\nChecks if <code>$file</code> exists in <code>env</code>.\n<pre>def local_exists(file):  test -e $file</pre>\n\n\n<code>$env_link(src, dst)</code>\nCreates a symbolic link at <code>$dst</code> that links to <code>$src</code>.\n<pre>def local_link(src, dst):  ln -s $src $dst</pre>\n\n\n<code>$env_touch(file)</code>\nCreates an empty file at path <code>$file</code>.\n<pre>def local_touch(file):  touch $file</pre>\n\n\n<code>$env_delete(file)</code>\nRemoves file <code>$file</code> in <code>env</code>.\n<pre>def local_delete(file):  rm $file</pre>"},{"location":"tutorial/filesys/#example-define-a-file-system-over-sftp","title":"Example: define a file system over SFTP","text":"<pre><code>import sftp\nobject my_server = sftp.Server(host=\"...\")\n</code></pre>"},{"location":"tutorial/filesys/#example-define-a-file-system-over-aws-s3","title":"Example: define a file system over AWS S3","text":"<pre><code>import aws\nobject my_bucket = aws.S3(name=\"...\")\n</code></pre>"},{"location":"tutorial/filesys/#transferring-file-between-environments","title":"Transferring file between environments","text":"<p>Sometimes different parts of a pipeline are run under different environments, \ne.g., data preprocessing may happen on a local machine, whereas training is done on an SSH grid, or \non AWS EC2 or Azure ML. Files are going to be copied to </p>\n<p>| <code>copy_from_$srcEnv_to_$dstEnv(src, dst)</code> | Copies a file <code>$src</code> on <code>srcEnv</code> to location <code>$dst</code> to <code>dstEnv</code>. | -                                                           |</p>"},{"location":"tutorial/hello/","title":"Runs a simple \"Hello, world\" task","text":"<p>First let's define our first task (without any inputs or outputs declared!):</p> <p>Note the syntax here: A code block starts after the <code>:</code> at the end of the task signature. A code block is a consecutive list of indented lines of scripts, where each line must start with at least 2 spaces.</p> <pre><code>task hello:\n  echo \"Hello, world!\"\n</code></pre> <p>Save this file as <code>hello.hm</code>. </p> <p>We have created our first Hypermake script file that contains a single task. Now let's run this task!</p> <p>Execute the following command in your shell:</p> <p>The basic command line usage is <code>hypermake &lt;script file&gt; &lt;subtask&gt; &lt;target&gt;</code>.</p> <p><pre><code>  hypermake hello.hm run hello \n</code></pre> We should see the output \"Hello, world!\" printed in the terminal.</p>"},{"location":"tutorial/modules/","title":"Classes/Objects","text":"<p>In Hypermake, sometimes it is needed to bundle certain definitions together so that it can be reused.  This forms an <code>object</code> (or modules, since a module can be seen as a singleton object):</p> <pre><code>object my_obj:\n  def x(...):\n  def y(...):\n</code></pre>"},{"location":"tutorial/packages/","title":"Packages","text":"<p>In Hypermake, packages are special tasks that builds a software package.  They can depends on other packages but not tasks, and will be built differently on different environments (see next tutorial).</p> <p>A package is defined as follows: <pre><code>package $packageName -&gt; $packageOutputName:\n  &lt;build script&gt;\n</code></pre> And to refer to a package, just use <code>$packageName</code>. For example, if a task requires a package <code>mypackage</code>, one can write <pre><code>task mytask(mypackage=$, ...) -&gt; out:\n  PYTHONPATH=$mypackage \\\n    python $mypackage/a/b/c.py --out=$out\n</code></pre></p>"},{"location":"tutorial/packages/#example-1-copying-a-package-from-a-local-directory","title":"Example 1: Copying a package from a local directory","text":"<pre><code>package pack1 -&gt; out:\n  ln -s $localDir out\n</code></pre>"},{"location":"tutorial/packages/#example-2-cloning-from-a-remote-repository","title":"Example 2: Cloning from a remote repository","text":"<pre><code>package pack2 -&gt; out:\n  git clone $repo out\n</code></pre>"},{"location":"tutorial/packages/#example-3-call-its-makefile-after-cloning-from-a-repo","title":"Example 3: Call its Makefile after cloning from a repo","text":"<pre><code>package pack3(repo=$) -&gt; out:\n  git clone $repo out\n  cd out\n  make\n</code></pre>"},{"location":"tutorial/packages/#example-4-creates-a-conda-environment-from-a-python-package","title":"Example 4: Creates a Conda environment from a Python package","text":"<pre><code>package pack4(pythonPackage=$) -&gt; out:\n  mkdir -p $out\n  conda env create -p $out -f $pythonPackage/environment.yml\n</code></pre>"},{"location":"tutorial/parameterized-compose/","title":"Composing parameterized tasks","text":""}]}